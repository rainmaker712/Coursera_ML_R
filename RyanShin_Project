---
title: "Project_Submission_R_RyanShin"
author: "Ryan Shin"
date: "2015년 11월 22일"
output: html_document
---

#Project Overview

This project is about to predict whether they did the exercise or not based on users data (6 participants). Data is from Jawbone up, Nike FuelBand, and Fitbit, IoT devices in these days and based on the belt, forearm, arm, and dumbell. Also, they were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

#Citation
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

#Data Input
Input Data from  http://groupware.les.inf.puc-rio.br/har.

```{r}
#Load Data

setwd("C:/Users/Ryan/Google Drive/코세라_R_머신러닝")
training <- "pml-training.csv"
testing <- "pml-testing.csv"

# Import the data and treating empty value as NA
df_training <- read.csv(training, na.strings=c("NA",""), header=TRUE)
colnames_train <- colnames(df_training)
df_testing <- read.csv(testing, na.strings=c("NA",""), header=TRUE)
colnames_test <- colnames(df_testing)



# Check column names (except classe and problem_id) are identical in the training and test set.
all.equal(colnames_train[1:length(colnames_train)-1], colnames_test[1:length(colnames_train)-1])
```

#Load Packages
```{r}
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)

install.packages("rattle")
install.packages("rpart.plot")
install.packages("randomForest")


```


#Preprocessing Data

Usually this takes most of time for data scientist. By figuring out unnecessary col. and NA values, it would be more efficient to figure out.

```{r}
#View(is.na(df_training))
#View(is.na(df_testing))

#Count How many missing data exisit in df_trainig Divide by Classe?
dim(df_training)
sapply(df_training, function(x) sum(is.na(x)))
summary(df_training$user_name)

#Use Amelia to see the missing data
require(Amelia)
missmap(df_training, main = "HAD(Human Activity Recognition) - Missing Data Map",
        col = c("yellow", "black"), legend = F)

#build a function for each col.
non_NA <- function(x) {
  as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}
col_num <- non_NA(df_training)
drops <- c() 
for (cnt in 1:length(col_num)) {
  if (col_num[cnt] < nrow(df_training)) {
    drops <- c(drops, colnames_train[cnt])
  }
}

  
#Drop and remove first 7 col.

df_training <- df_training[,!(names(df_training) %in% drops)]
df_training <- df_training[,8:length(colnames(df_training))]

df_testing <- df_testing[,!(names(df_testing) %in% drops)]
df_testing <- df_testing[,8:length(colnames(df_testing))]

df_train_colnames <- colnames(df_training) #show the col only to think
df_test_colnames <- colnames(df_testing)

# Check column names are identical in the training and test set.
all.equal(df_train_colnames[1:length(df_train_colnames)-1], df_test_colnames[1:length(df_test_colnames)-1])

#Check to figure out nearZerovar
nzv <- nearZeroVar(df_training, saveMetrics = T)

#Remove variables with more than 70% NA

df_training2 <- df_training
for(i in 1:length(df_training)) {
    if( sum( is.na( df_training[, i] ) ) /nrow(df_training) >= .7) {
        for(j in 1:length(df_training2)) {
            if( length( grep(names(df_training[i]), names(df_training2)[j]) ) == 1)  {
                df_training2 <- df_training2[ , -j]
            }   
        } 
    }
}

View(df_training2)

# Set back to the original variable name
```


#Data Exploration
**To find the pattern of df-training & df-testing dataset.**

> 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects.
Class A: Exactly according to the specification
Class B: throwing the elbows to the front
Class C: lifting the dumbbell only halfway
Class D: lowering the dumbbell only halfway
Class E; and throwing the hips to the front (Class E).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3sCeq4XbN
![Overview of data category](http://groupware.les.inf.puc-rio.br/static/WLE/on-body-sensing-schema.png)

*Divide dataset 60:40 (commmon ratio)*

```{r}
inTrain <- createDataPartition(y=df_training$classe, p=0.6, list=FALSE)
training <- df_training[inTrain, ]; testing <- df_training[-inTrain,] #Subset data
```


#Algorithm
```{r}
# Divide the given training set into 4 roughly equal sets.
set.seed(666)
ids_small <- createDataPartition(y=df_training$classe, p=0.25, list=FALSE)
df_small1 <- df_training[ids_small,]
df_remainder <- df_training[-ids_small,]
set.seed(666)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.33, list=FALSE)
df_small2 <- df_remainder[ids_small,]
df_remainder <- df_remainder[-ids_small,]
set.seed(666)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.5, list=FALSE)
df_small3 <- df_remainder[ids_small,]
df_small4 <- df_remainder[-ids_small,]
# Divide each of these 4 sets into training (60%) and test (40%) sets.
set.seed(666)
inTrain <- createDataPartition(y=df_small1$classe, p=0.6, list=FALSE)
df_small_training1 <- df_small1[inTrain,]
df_small_testing1 <- df_small1[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small2$classe, p=0.6, list=FALSE)
df_small_training2 <- df_small2[inTrain,]
df_small_testing2 <- df_small2[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small3$classe, p=0.6, list=FALSE)
df_small_training3 <- df_small3[inTrain,]
df_small_testing3 <- df_small3[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small4$classe, p=0.6, list=FALSE)
df_small_training4 <- df_small4[inTrain,]
df_small_testing4 <- df_small4[-inTrain,]
```

#Eval

```{r}
# Train on training set 1 of 4 with no extra features.
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., data = df_small_training1, method="rpart")
print(modFit, digits=3)
print(modFit$finalModel, digits=3)

```


```{r}
fancyRpartPlot(modFit$finalModel)
# Run against testing set 1 of 4 with no extra features.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

# Train on training set 1 of 4 with only preprocessing.
set.seed(666)
modFit <- train(df_small_training1$classe ~ .,  preProcess=c("center", "scale"), data = df_small_training1, method="rpart")
print(modFit, digits=3)

# Train on training set 1 of 4 with both preprocessing and cross validation.
set.seed(666)
modFit <- train(df_small_training1$classe ~ .,  preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data = df_small_training1, method="rpart")
print(modFit, digits=3)

# Run against testing set 1 of 4 with both preprocessing and cross validation.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

```

Random Forest
```{r}
# Train on training set 1 of 4 with only cross validation.
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)

# Run against testing set 1 of 4.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))

# Train on training set 1 of 4 with only both preprocessing and cross validation.
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)

# Run against testing set 1 of 4.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))

# Train on training set 2 of 4 with only cross validation.
set.seed(666)
modFit <- train(df_small_training2$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training2)
print(modFit, digits=3)

# Run against testing set 2 of 4.
predictions <- predict(modFit, newdata=df_small_testing2)
print(confusionMatrix(predictions, df_small_testing2$classe), digits=4)

# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))

# Train on training set 4 of 4 with only cross validation.
set.seed(666)
modFit <- train(df_small_training4$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training4)
print(modFit, digits=3)

# Run against testing set 4 of 4.
predictions <- predict(modFit, newdata=df_small_testing4)
print(confusionMatrix(predictions, df_small_testing4$classe), digits=4)

# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))

```


Out of Sample Error

According to Professor Leek's Week 1 “In and out of sample errors”, the out of sample error is the “error rate you get on new data set.” In my case, it's the error rate after running the predict() function on the 4 testing sets:

Random Forest (preprocessing and cross validation) Testing Set 1: 1 - .9714 = 0.0286
Random Forest (preprocessing and cross validation) Testing Set 2: 1 - .9634 = 0.0366
Random Forest (preprocessing and cross validation) Testing Set 3: 1 - .9655 = 0.0345
Random Forest (preprocessing and cross validation) Testing Set 4: 1 - .9563 = 0.0437
Since each testing set is roughly of equal size, I decided to average the out of sample error rates derived by applying the random forest method with both preprocessing and cross validation against test sets 1-4 yielding a predicted out of sample rate of 0.03585.

CONCLUSION

I received three separate predictions by appling the 4 models against the actual 20 item training set:

A) Accuracy Rate 0.0286 Predictions: B A A A A E D B A A B C B A E E A B B B

B) Accuracy Rates 0.0366 and 0.0345 Predictions: B A B A A E D B A A B C B A E E A B B B

C) Accuracy Rate 0.0437 Predictions: B A B A A E D D A A B C B A E E A B B B

Since Professor Leek is allowing 2 submissions for each problem, I decided to attempt with the two most likely prediction sets: option A and option B.

Since options A and B above only differed for item 3 (A for option A, B for option B), I subimitted one value for problems 1-2 and 4-20, while I submitted two values for problem 3. For problem 3, I was expecting the automated grader to tell me which answer (A or B) was correct, but instead the grader simply told me I had a correct answer. All other answers were also correct, resulting in a score of 100%.
